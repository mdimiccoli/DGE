{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "#from dge_fun import *\n",
    "#from kmeans import lloyd\n",
    "\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# FORCE CPU for timing purpose\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "#load cleaned data\n",
    "path_dir = \"\"\n",
    "save_path = \"\"\n",
    "lossdata_path = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:151: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:151: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<ipython-input-4-c54c7a38bf28>:151: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if mm is not 0:\n"
     ]
    }
   ],
   "source": [
    "#--- FUNCTION DEFINITONS --> PUT INTO MODULE\n",
    "#--- ? HOW TO PASS ON \"device\" TO MODULE ?\n",
    "\n",
    "def class_sim_torch(X):\n",
    "    Tpxy=torch.sum(torch.abs(X - X[:, None]),2)\n",
    "    return Tpxy\n",
    "\n",
    "def PDIST(X):\n",
    "    #print(X.type())\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    numel=N*N*dim\n",
    "    numel_max = 400000000\n",
    "    #numel_max = 100000000\n",
    "    nl=np.ceil(numel/numel_max).astype(int) # divide dim by this\n",
    "    if nl ==1:\n",
    "        Tpxy=torch.sum(torch.abs(X - X[:, None]),2)\n",
    "    else:\n",
    "        Tpxy=torch.zeros(N,N).to(device)\n",
    "        Tpxy=Tpxy.type(X.type())\n",
    "        mdim=np.floor(dim/nl).astype(int) # dim slices\n",
    "        for i in range(nl+1):\n",
    "            i1=i*mdim\n",
    "            i2=np.minimum((i+1)*mdim,dim)\n",
    "            if i1<i2:\n",
    "                Xt=X[:,i1:i2]\n",
    "                Tpxy.add_(torch.sum(torch.abs(Xt - Xt[:, None]),2))    \n",
    "    return Tpxy\n",
    "\n",
    "def NLSS( X, param,r_loc):\n",
    "\n",
    "    if len(sys.argv)<3:\n",
    "        r_loc=[]\n",
    "        \n",
    "    N = len(X)\n",
    "    dim = X.shape[1]\n",
    "    #--- compute distance & similarity\n",
    "    XM2 = sp.spatial.distance.squareform(sp.spatial.distance.pdist(X,metric = 'cityblock'))/dim\n",
    "    XSS0 = np.exp(-1/param*XM2)\n",
    "\n",
    "    # if not NLM over whole sequence, set similarity beyond search radius to\n",
    "    # zero and extend boundaries\n",
    "    if r_loc:   \n",
    "        t=np.arange(1,N)\n",
    "        for i in range(1,N):\n",
    "            t = np.concatenate((t, np.arange(1,N-i)), axis=None)\n",
    "        t = np.transpose(t)\n",
    "        T=spatial.distance.squareform(t) #%T=T(:,1:min(N1,N2));\n",
    "        MASK=sp.exp(math.log(0.25)*T/r_loc)\n",
    "        XSS=np.multiply(XSS0,MASK)\n",
    "        XSS = np.transpose(XSS)\n",
    "        XSS= np.concatenate((np.flipud(XSS[0:r_loc,:]),XSS,np.flipud(XSS[XSS.shape[0]-r_loc:XSS.shape[0],:])), axis=0)\n",
    "        XSS = np.transpose(XSS)\n",
    "    return XSS\n",
    "\n",
    "def construct_SS_matrix(X,nnh):\n",
    "# function [XX] = construct_SS_matrix(X,nnh)\n",
    "# \n",
    "# stack nnh left-and right neighbor feature vectors of X using periodic\n",
    "# boundary conditions.  \n",
    "# size(X)  = N_frames x N_features \n",
    "# size(XX) = N_frames x N_features* 2*nnh \n",
    "\n",
    "    # periodic border effects\n",
    "    XE=np.concatenate((  np.flipud(X[1:nnh+1,:]),X,(np.flipud(X[X.shape[0]-nnh-2:X.shape[0]-1,:]))), axis = 0)\n",
    "    \n",
    "    #--- construct neighborhood \n",
    "    a= np.arange(-nnh,0, dtype = int)\n",
    "    b = np.arange(1,nnh+1, dtype = int)\n",
    "\n",
    "    NH_ID = np.concatenate((a,b))# do not include self\n",
    "    #NH_ID = np.concatenate((a,0, b))% include self (i.e., \"center pixel\")\n",
    "\n",
    "    if not NH_ID.any():\n",
    "        # nothing to do: working with pixel only, without neighbors\n",
    "        XX=X\n",
    "    else:\n",
    "        # stack neighbors\n",
    "        XX=np.array([]) \n",
    "    for inh in NH_ID:\n",
    "        XTMP= np.roll(XE,inh,axis=0) \n",
    "        if not XX.size:  \n",
    "            XX = XTMP[nnh:XTMP.shape[0]-nnh-1,:]\n",
    "        else:\n",
    "            XX=np.hstack((XX,XTMP[nnh:XTMP.shape[0]-nnh-1,:]))\n",
    "    return XX\n",
    "\n",
    "def NLSS_torch( X, param,r_loc):\n",
    "\n",
    "    if len(sys.argv)<3:\n",
    "        r_loc=[]\n",
    "        \n",
    "    N = X.size()[0]\n",
    "    dim = X.size()[1]\n",
    "    #--- compute distance & similarity\n",
    "    XM2 = PDIST(X)/dim\n",
    "    XSS0 = torch.exp(-1/param*XM2).double()\n",
    "    # if not NLM over whole sequence, set similarity beyond search radius to\n",
    "    # zero and extend boundaries\n",
    "    if r_loc:   \n",
    "        t1=torch.zeros(N,1).to(device)\n",
    "        t1[:,0]=torch.arange(0,N)\n",
    "        T=PDIST(t1)\n",
    "        mparam=-1.3862943611198906/r_loc\n",
    "        MASK=torch.exp(mparam*T).double()\n",
    "        XSS=torch.mul(XSS0,MASK)\n",
    "        XSS = XSS.transpose(1,0)\n",
    "        \n",
    "        np1=torch.flip(XSS[0:r_loc,:],(0,))        \n",
    "        np2=torch.flip(XSS[XSS.size()[0]-r_loc:XSS.size()[0],:],(0,))\n",
    "        XSS = torch.cat((np1, XSS, np2))\n",
    "        XSS = XSS.transpose(1,0)\n",
    "    return XSS\n",
    "\n",
    "def PCA(X, k=2):\n",
    "    # preprocess the data\n",
    "    X_mean = torch.mean(X,0)\n",
    "    X = X - X_mean.expand_as(X)\n",
    "    # svd\n",
    "    U,S,V = torch.svd(torch.t(X))\n",
    "    return U[:,:k], torch.mm(X,U[:,:k])\n",
    "\n",
    "def distx_fun(X):\n",
    "    N = X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "\n",
    "    Z = torch.mm(X,X.t())#.to(device)\n",
    "    djj = torch.sqrt(torch.diag(Z))*torch.ones(1,N).double().t().to(device)\n",
    "    Z = torch.div(1 - torch.div(torch.div(Z,djj.t()), djj) , dim)\n",
    "    return Z\n",
    "    \n",
    "def simz_fun(Z,param):\n",
    "    G = torch.exp(torch.mul(-1/param,Z)) \n",
    "    N=G.size()[1]\n",
    "    z = torch.sum(G,0)\n",
    "    G=torch.div(G,z.repeat(N,1));\n",
    "    return G\n",
    "\n",
    "def norm11(X):\n",
    "    for i in range(0,X.size()[1]):\n",
    "        X[:,i]=X[:,i]-X[:,i].min()\n",
    "        mm=X[:,i].max()\n",
    "        if mm is not 0:\n",
    "            X[:,i]=X[:,i]/mm*2-1\n",
    "    return X\n",
    "\n",
    "def loss_fun(G,W):\n",
    "    N=G.size()[0]\n",
    "    # cross-entropy loss \n",
    "    w=W.contiguous().view(N*N,-1)\n",
    "    g=G.contiguous().view(N*N,-1)\n",
    "    L=-torch.sum(torch.mul(w,torch.log(g)))\n",
    "    L=L+torch.sum(torch.mul(w,torch.log(w)))\n",
    "    return L\n",
    "\n",
    "def d_distx_fun(X):\n",
    "    tt=X.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[1]\n",
    "\n",
    "    Z = torch.mm(X,X.t()) + 1e-16\n",
    "    N = Z.size()[1];\n",
    "    djj = torch.sqrt(torch.diag(Z))*torch.ones(1,N).double().t().to(device)\n",
    "    dii= djj.t()\n",
    "    t1 = torch.div(torch.div(1,dii), djj)\n",
    "    t1 = t1 + torch.diag(torch.diag(t1)) # OK\n",
    "    t2 = t1*torch.div(torch.div(Z,djj), djj) # OK\n",
    "\n",
    "    X1=X\n",
    "    X1.detach().clone()\n",
    "    X1=X1.repeat((N,1,1))\n",
    "    X2=X1\n",
    "    X2.detach().clone()\n",
    "    X2=X2.transpose(0,1)\n",
    "\n",
    "    T1=t1.repeat((dim,1,1))\n",
    "    T1=T1.transpose(0,1).transpose(1,2)\n",
    "    T2=t2.repeat((dim,1,1))\n",
    "    T2=T2.transpose(0,1).transpose(1,2)\n",
    "\n",
    "    #dZ=X2*T1 - X1*T2\n",
    "    dZ=torch.mul(X2,T1) - torch.mul(X1,T2)\n",
    "    dZ=-dZ\n",
    "    return dZ \n",
    "    \n",
    "def d_simz_fun(Z,dZ,param):\n",
    "    tt=dZ.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[2]\n",
    "    G = torch.exp(torch.mul(-1/param,Z))\n",
    "    dG0=-1/param*G\n",
    "    dG0=dG0.repeat((dim,1,1))\n",
    "    dG0=dG0.transpose(0,1).transpose(1,2)\n",
    "    #dG=dZ*dG0\n",
    "    dG=torch.mul(dZ,dG0)\n",
    "\n",
    "    sG=torch.sum(G,0).repeat((N,1)).repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    sdG=torch.sum(dG,0).repeat((N,1,1))\n",
    "\n",
    "    repG=G.repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    dG=torch.div((dG*sG - repG*sdG) , (sG*sG))\n",
    "    return dG\n",
    "\n",
    "def d_loss_fun(G,dG,W):\n",
    "    tt=dG.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[2]\n",
    "    WG=torch.div(W,G).repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    dL=-torch.sum(dG*WG,0)\n",
    "    return dL\n",
    "\n",
    "def full_grad_fun(X,W,param):\n",
    "    Z = distx_fun(X)\n",
    "    dZ = d_distx_fun(X)\n",
    "    G = simz_fun(Z,param)\n",
    "    dG = d_simz_fun(Z,dZ,param)\n",
    "    dL = d_loss_fun(G,dG,W)\n",
    "    return dL, G\n",
    "\n",
    "def sim_grad_descent(X,W,param,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    dL,G=full_grad_fun(X,W,param) # compute gradient\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    dataiter=X-eta*dL;\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        dL,G=full_grad_fun(dataiter,W,param) # compute gradient\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=dataiter.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2;\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        dataiter=dataiter - eta*dL\n",
    "    \n",
    "    return dataiter\n",
    "\n",
    "def sim_grad_descent_autograd(X,W,param,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    \n",
    "    loss=loss_fun(simz_fun(distx_fun(X),param),W)\n",
    "    loss.backward()\n",
    "    dL=X.grad\n",
    "\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    X=X-eta*dL\n",
    "    X=X.detach().clone()\n",
    "    X.requires_grad=True\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        loss=loss_fun(simz_fun(distx_fun(X),param),W)\n",
    "        loss.backward()\n",
    "#        dL=X.grad.detach().clone()\n",
    "        dL=X.grad\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=X.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        X=X - eta*dL\n",
    "        X=X.detach().clone()\n",
    "        X.requires_grad=True\n",
    "    \n",
    "    return X\n",
    "\n",
    "def sim_grad_descent_biW(X,W1,W2,param,aa,niter):\n",
    "    eta0=1e-6\n",
    "    aa=min(max(aa,0),1)\n",
    "    a1,a2=aa,1-aa\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    dL1,G1=full_grad_fun(X,W1,param) # compute gradient\n",
    "    dL2,G2=full_grad_fun(X,W2,param) # compute gradient\n",
    "    dL=a1*dL1+a2*dL2\n",
    "    G=a1*G1+a2*G2\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    dataiter=X-eta*dL;\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        dL1,G1=full_grad_fun(dataiter,W1,param) # compute gradient\n",
    "        dL2,G2=full_grad_fun(dataiter,W2,param) # compute gradient\n",
    "        dL=a1*dL1+a2*dL2\n",
    "        G=a1*G1+a2*G2\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=dataiter.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2;\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        dataiter=dataiter - eta*dL\n",
    "    \n",
    "    return dataiter\n",
    "\n",
    "def loss_fun_biW(G,W1,W2,aa):\n",
    "    N=G.size()[0]\n",
    "    aa=min(max(aa,0),1)\n",
    "    a1,a2=aa,1-aa\n",
    "    # cross-entropy loss \n",
    "    w1=W1.contiguous().view(N*N,-1)\n",
    "    w2=W2.contiguous().view(N*N,-1)\n",
    "    g=G.contiguous().view(N*N,-1)\n",
    "    L=-a1*torch.sum(torch.mul(w1,torch.log(g)))-a2*torch.sum(torch.mul(w2,torch.log(g)))\n",
    "    L=L+a1*torch.sum(torch.mul(w1,torch.log(w1)))+a2*torch.sum(torch.mul(w2,torch.log(w2)))\n",
    "    return L\n",
    "\n",
    "def sim_grad_descent_biW_autograd(X,W1,W2,param,aa,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    \n",
    "    loss=loss_fun_biW(simz_fun(distx_fun(X),param),W1,W2,aa)\n",
    "    loss.backward()\n",
    "    dL=X.grad\n",
    "\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    X=X-eta*dL\n",
    "    X=X.detach().clone()\n",
    "    X.requires_grad=True\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        loss=loss_fun_biW(simz_fun(distx_fun(X),param),W1,W2,aa)\n",
    "        loss.backward()\n",
    "        dL=X.grad\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=X.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        X=X - eta*dL\n",
    "        X=X.detach().clone()\n",
    "        X.requires_grad=True\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'baseline_agg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-da2319febbf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaseline_agg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_aggregation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_prediction_signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margrelmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'baseline_agg'"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import h5py\n",
    "from baseline_agg import mean_aggregation, get_dist, get_prediction_signal\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.signal import argrelmax\n",
    "from testing_utils import evaluation_measures\n",
    "all_scores = {}\n",
    "scores = evaluation_measures(win=5)\n",
    "\n",
    "dataset = \"/home/hwendt/jupyter_MD/ACMMM2019/CES/EDUB-Seg.h5\"\n",
    "\n",
    "path_dir = \"/media/DATA/DATASET/EDUB-Seg/\"\n",
    "\n",
    "\n",
    "f = h5py.File(dataset, 'r') \n",
    "user = f['user_id']\n",
    "day = f['day']\n",
    "gt = f['boundary']\n",
    "\n",
    "samples = pd.DataFrame(np.vstack((user, day)).T)\n",
    "samples = samples.drop_duplicates().values\n",
    "i=1\n",
    "for u, d in samples:\n",
    "    mask = (user[:]==u)*(day[:]==d)\n",
    "    GT=gt[mask,]\n",
    "    #np.savetxt(path_dir + \"GT/h5_Subject\" + str(u) + \"_Set\"+str(d+1)+\".txt\",GT)\n",
    "    np.save(path_dir + \"GT/seq_\" + str(i),GT)\n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "\n",
    "def mean_aggregation(feat, N=10):\n",
    "    C = [np.mean(feat[(i-N):i], axis=0) for\n",
    "         i in range(N, len(feat)+1)]\n",
    "    P = np.vstack(([np.mean(feat[:i], axis=0) for\n",
    "                    i in range(1, N)], C))\n",
    "    F = np.vstack((C, \n",
    "                   [np.mean(feat[(-N + i):], axis=0) for\n",
    "                    i in range(1, N)]))\n",
    "    return P, F\n",
    "\n",
    "\n",
    "def boundary_prediction(embedded_seqFfromP, embedded_seqPfromF,\n",
    "                        order=5, level=1):\n",
    "    \"\"\"Predicts the event boundaries at time t given\n",
    "    the past and future visual context.\n",
    "    Inputs:\n",
    "        visual context given the past\n",
    "        visual context given the future\n",
    "        order=size of the window for the local maxima  \n",
    "        level=hierarchy level for the event segmentation\n",
    "    Output:\n",
    "        prediction vector (1/0)\n",
    "    \"\"\"\n",
    "    # at time t, we want the difference between context from the past at t-1\n",
    "    # and context from the future at t+1:\n",
    "    signal = np.array([cosine(a, b) for\n",
    "                       (a, b) in zip(embedded_seqFfromP[:-2],\n",
    "                                     embedded_seqPfromF[2:])])\n",
    "\n",
    "    # First and last context predictions will be noisy, better to ignore:\n",
    "    signal[0] = 0\n",
    "    signal[-1] = 0\n",
    "\n",
    "    local_max = argrelmax(signal, order=order)[0]\n",
    "    th = np.mean(signal[local_max])\n",
    "    if level != 1:\n",
    "        th += (1-level)*np.std(signal[local_max])\n",
    "    prediction = np.zeros(len(signal))\n",
    "    prediction[local_max[signal[local_max] > th]] = 1\n",
    "\n",
    "    # First and last frames are always event boundaries \n",
    "    # (context prediction goes from t=1 to t=N-1):\n",
    "    return np.hstack((1, prediction, 1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "FPTH = \"\"\n",
    "SPTH = FPTH + 'NL_SS_FINAL/self_loc_L1_'\n",
    "\n",
    "\n",
    "SAVEDIR = \"\"\n",
    "\n",
    "N_SEQ=20\n",
    "SNME='EDUB-Seg'\n",
    "\n",
    "# master parameter for similarity\n",
    "par00=0.25 \n",
    "\n",
    "# search radius and neighborhood for NLmean\n",
    "r_loc=3\n",
    "nnh=1\n",
    "NLSstr= '_NH' + str(nnh) + '_R' + str(r_loc) + '_'+str(par00)\n",
    "\n",
    "# parameters for initial embedding\n",
    "param2=par00/0.8\n",
    "param1=par00/50/2\n",
    "d_emb = 15\n",
    "niter_e=150\n",
    "embstr = '_emb_dim' + str(d_emb)\n",
    "\n",
    "# parameters for dynamic graph embedding\n",
    "N_iter_G=10 # number of DGE iterations\n",
    "niter_g=5  # embedding steps\n",
    "aa=0.3     # proportionality factor for initial matrix W0\n",
    "aa2=1e-3   # strengthen diagonal\n",
    "NCL=9      # number of clusters\n",
    "Cdamp=0.95 # reduction of out-of-cluster similarity\n",
    "\n",
    "\n",
    "aa=0.3; aa2=3e-2; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.975;\n",
    "aa2=3e-1;\n",
    "\n",
    "aa=0.1; aa2=5e-1; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.85;\n",
    "\n",
    "\n",
    "## FROM GRID SEARCH\n",
    "aa=0.4; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.9; NF = 1;\n",
    "#_a0.4_t0.3_NF1_CL10_CLd0.9_ie5\n",
    "\n",
    "aa=0.3; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=3; Cdamp=0.9; NF = 2;\n",
    "#_a0.3_t0.3_NF2_CL10_CLd0.9_ie3\n",
    "\n",
    "aa=0.1; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=3; Cdamp=0.9; NF = 3;\n",
    "#_a0.1_t0.3_NF3_CL10_CLd0.9_ie3\n",
    " \n",
    "\n",
    "\n",
    "SAVE_OUT=0\n",
    "\n",
    "DO_AUTOGRAD=0 # Use Autograd instead of manual grad\n",
    "DO_PCA=0 # initialize embedding with PCA\n",
    "\n",
    "#KMEANS_sklearn=0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# FORCE CPU for timing purpose\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "iNF=int(NF)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_measures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9ed7df4f0240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mMAXW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_measures' is not defined"
     ]
    }
   ],
   "source": [
    "# EVALUATE PERFORMANCE OF DGE\n",
    "TOL=5\n",
    "import itertools\n",
    "\n",
    "\n",
    "bestIt=1; bestWin=8;\n",
    "\n",
    "all_scores = {}\n",
    "scores = evaluation_measures(win=5)\n",
    "\n",
    "MAXW=10\n",
    "\n",
    "myFM=np.zeros( (N_SEQ,N_iter_G,MAXW) )\n",
    "myPR=np.zeros( (N_SEQ,N_iter_G,MAXW) )\n",
    "myRE=np.zeros( (N_SEQ,N_iter_G,MAXW) )\n",
    "\n",
    "for subid in range(N_SEQ): \n",
    "    GT = np.load(path_dir + \"GT/seq_\" + str(subid+1)+\".npy\")\n",
    "    start_time1 = time.time()\n",
    "    \n",
    "    #--- LOAD EDUB features\n",
    "    FILENAME= FPTH + SNME\n",
    "    FN= SNME + '_seq' + str(subid+1) \n",
    "    FN_DGE= SNME + '_seq' + str(subid+1) + '_DGE'\n",
    "    print(FN)\n",
    "\n",
    "    #######################################################\n",
    "    #--- PART 1: NLSS : compute similarity and do nonlocal mean\n",
    "    FN1 = SAVEDIR + FN + NLSstr + '.npy' \n",
    "    GST = \"_man\"\n",
    "    \n",
    "    PCAstr = \"\"\n",
    "    FN2 = SAVEDIR + FN + NLSstr + embstr + GST + PCAstr + '.npy'\n",
    "\n",
    "    #######################################################\n",
    "    #--- PART 3: DGE LOOP\n",
    "    \n",
    "    \n",
    "    \n",
    "    for nDGE in range(N_iter_G):\n",
    "        \n",
    "        FN3 = SAVEDIR + FN + NLSstr + embstr + '_it' + str(niter_e) + GST + PCAstr + '_it' + str(nDGE+1) + '_DGE.npy'\n",
    "\n",
    "        if iNF>1:\n",
    "            FN3 = SAVEDIR + FN + NLSstr + embstr + '_it' + str(niter_e)  + GST +'_NF' + str(iNF) + PCAstr + '_it' + str(nDGE+1) + '_DGE.npy'; \n",
    "        \n",
    "\n",
    "        \n",
    "        dataiter = np.load(FN3)\n",
    "        \n",
    "        if nDGE == bestIt:\n",
    "            FN4 = SAVEDIR + 'DGE_FINAL_FEATURES/' + FN_DGE + '.npy'\n",
    "            np.save(FN4,dataiter)\n",
    "            print(FN4)\n",
    "        \n",
    "#        print(dataiter.shape)\n",
    "        \n",
    "        for nwindow in range(2,MAXW):\n",
    "            P, F = mean_aggregation(dataiter, N=nwindow)\n",
    "            prediction =  boundary_prediction(P,F)\n",
    "            scores.reset(y_pred=prediction, y_gt=GT, win = TOL)            \n",
    "            myFM[subid,nDGE,nwindow]=scores.f_measure()\n",
    "            myPR[subid,nDGE,nwindow]=scores.get_precision()\n",
    "            myRE[subid,nDGE,nwindow]=scores.get_recall()\n",
    "            \n",
    "            \n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "meanFM=myFM.mean(axis=0)\n",
    "meanPR=myPR.mean(axis=0)\n",
    "meanRE=myRE.mean(axis=0)\n",
    "\n",
    "maxFM=meanFM.max(); \n",
    "#idM1=meanFM.argmax(axis=0); idM2=meanFM[idM1,:].argmax(axis=1);\n",
    "idM1,idM2 = np.where(meanFM==meanFM.max())\n",
    "maxPR=meanPR[idM1,idM2][0]\n",
    "maxRE=meanRE[idM1,idM2][0]\n",
    "\n",
    "print(idM1[0],idM2[0])\n",
    "print(maxFM,maxPR,maxRE)\n",
    "\n",
    "\n",
    "# PRINT FOR FIXED/ALL WINDOW AND ITERATION\n",
    "bestIt=1; bestWin=8;\n",
    "print(bestIt,bestWin)\n",
    "print(meanFM[bestIt,bestWin],meanPR[bestIt,bestWin],meanRE[bestIt,bestWin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8\n",
      "0.6975119826831833 0.7214731356059618 0.7027570148456753\n",
      "1 8\n",
      "0.6975119826831833 0.7214731356059618 0.7027570148456753\n"
     ]
    }
   ],
   "source": [
    "meanFM=myFM.mean(axis=0)\n",
    "meanPR=myPR.mean(axis=0)\n",
    "meanRE=myRE.mean(axis=0)\n",
    "\n",
    "maxFM=meanFM.max(); \n",
    "#idM1=meanFM.argmax(axis=0); idM2=meanFM[idM1,:].argmax(axis=1);\n",
    "idM1,idM2 = np.where(meanFM==meanFM.max())\n",
    "maxPR=meanPR[idM1,idM2][0]\n",
    "maxRE=meanRE[idM1,idM2][0]\n",
    "\n",
    "print(idM1[0],idM2[0])\n",
    "print(maxFM,maxPR,maxRE)\n",
    "\n",
    "\n",
    "# PRINT FOR FIXED/ALL WINDOW AND ITERATION\n",
    "bestIt=1; bestWin=8;\n",
    "print(bestIt,bestWin)\n",
    "print(meanFM[bestIt,bestWin],meanPR[bestIt,bestWin],meanRE[bestIt,bestWin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FN + NLSstr + embstr + '_it' + str(niter_e) + GST + PCAstr + '_it' + str(nDGE+1) + '_DGE.npy')\n",
    "print(FN3)\n",
    "print(maxFM,maxPR,maxRE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM\n",
      "[0.65529762 0.69751198 0.68165592 0.67126756 0.66236916 0.65233191\n",
      " 0.64454644 0.64448353 0.63596542 0.6295629 ]\n",
      "PR\n",
      "[0.68776772 0.72147314 0.70686499 0.68882751 0.67781854 0.66507126\n",
      " 0.66372263 0.66268959 0.65997504 0.65501122]\n",
      "RE\n",
      "[0.65896458 0.70275701 0.68856242 0.68777862 0.6832521  0.67588988\n",
      " 0.6692537  0.67036481 0.66251225 0.6572062 ]\n"
     ]
    }
   ],
   "source": [
    "print('FM')\n",
    "print(meanFM[:,bestWin])\n",
    "print('PR')\n",
    "print(meanPR[:,bestWin])\n",
    "print('RE')\n",
    "print(meanRE[:,bestWin])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT FOR ALL SUBJECTS\n",
    "print(np.transpose([np.transpose(myFM[:,idM1,idM2])[0],np.transpose(myPR[:,idM1,idM2])[0],np.transpose(myRE[:,idM1,idM2])[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDUB-Seg_seq1\n",
      "EDUB-Seg_seq2\n",
      "EDUB-Seg_seq3\n",
      "EDUB-Seg_seq4\n",
      "EDUB-Seg_seq5\n",
      "EDUB-Seg_seq6\n",
      "EDUB-Seg_seq7\n",
      "EDUB-Seg_seq8\n",
      "EDUB-Seg_seq9\n",
      "EDUB-Seg_seq10\n",
      "EDUB-Seg_seq11\n",
      "EDUB-Seg_seq12\n",
      "EDUB-Seg_seq13\n",
      "EDUB-Seg_seq14\n",
      "EDUB-Seg_seq15\n",
      "EDUB-Seg_seq16\n",
      "EDUB-Seg_seq17\n",
      "EDUB-Seg_seq18\n",
      "EDUB-Seg_seq19\n",
      "EDUB-Seg_seq20\n",
      "--- 62.58055138587952 seconds --------\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE PERFORMANCE OF NLSS and EMBEDDING\n",
    "\n",
    "TOL = 5\n",
    "\n",
    "import itertools\n",
    "start_time = time.time()\n",
    "\n",
    "all_scores = {}\n",
    "scores = evaluation_measures(win=5)\n",
    "\n",
    "MAXW=10\n",
    "\n",
    "myFM0=np.zeros( (N_SEQ,MAXW) )\n",
    "myPR0=np.zeros( (N_SEQ,MAXW) )\n",
    "myRE0=np.zeros( (N_SEQ,MAXW) )\n",
    "\n",
    "myFM1=np.zeros( (N_SEQ,MAXW) )\n",
    "myPR1=np.zeros( (N_SEQ,MAXW) )\n",
    "myRE1=np.zeros( (N_SEQ,MAXW) )\n",
    "\n",
    "myFM2=np.zeros( (N_SEQ,MAXW) )\n",
    "myPR2=np.zeros( (N_SEQ,MAXW) )\n",
    "myRE2=np.zeros( (N_SEQ,MAXW) )\n",
    "\n",
    "for subid in range(N_SEQ): \n",
    "    GT = np.load(path_dir + \"GT/seq_\" + str(subid+1)+\".npy\")\n",
    "    start_time1 = time.time()\n",
    "    \n",
    "    #--- LOAD EDUB features\n",
    "    FILENAME= FPTH + SNME\n",
    "    FN= SNME + '_seq' + str(subid+1) \n",
    "    print(FN)\n",
    "\n",
    "    dataiter = np.load(FILENAME + '_seq' + str(subid+1) + '.npy')\n",
    "    \n",
    "    for nwindow in range(2,MAXW):\n",
    "        P, F = mean_aggregation(dataiter, N=nwindow)\n",
    "        prediction =  boundary_prediction(P,F)\n",
    "        scores.reset(y_pred=prediction, y_gt=GT, win = TOL)            \n",
    "        myFM0[subid,nwindow]=scores.f_measure()\n",
    "        myPR0[subid,nwindow]=scores.get_precision()\n",
    "        myRE0[subid,nwindow]=scores.get_recall()\n",
    "        \n",
    "    #######################################################\n",
    "    #--- PART 1: NLSS : compute similarity and do nonlocal mean\n",
    "    FN1 = SAVEDIR + FN + NLSstr + '.npy' \n",
    "    GST = \"_man\"\n",
    "    \n",
    "    dataiter = np.load(FN1)        \n",
    "    for nwindow in range(2,MAXW):\n",
    "        P, F = mean_aggregation(dataiter, N=nwindow)\n",
    "        prediction =  boundary_prediction(P,F)\n",
    "        scores.reset(y_pred=prediction, y_gt=GT, win = TOL)            \n",
    "        myFM1[subid,nwindow]=scores.f_measure()\n",
    "        myPR1[subid,nwindow]=scores.get_precision()\n",
    "        myRE1[subid,nwindow]=scores.get_recall()\n",
    "        \n",
    "        if nwindow==MAXW-1:\n",
    "            FN= SNME + '_seq' + str(subid+1)\n",
    "            np.save(SAVEDIR + FN + '_NLSS_P' + '.npy' ,P)\n",
    "            np.save(SAVEDIR + FN + '_NLSS_F' + '.npy' ,F)\n",
    "            \n",
    "    \n",
    "    PCAstr = \"\"\n",
    "    FN2 = SAVEDIR + FN + NLSstr + embstr + '_it' + str(niter_e) + GST + PCAstr + '.npy'\n",
    "    dataiter = np.load(FN2)        \n",
    "    for nwindow in range(2,MAXW):\n",
    "        P, F = mean_aggregation(dataiter, N=nwindow)\n",
    "        prediction =  boundary_prediction(P,F)\n",
    "        scores.reset(y_pred=prediction, y_gt=GT, win = TOL)            \n",
    "        myFM2[subid,nwindow]=scores.f_measure()\n",
    "        myPR2[subid,nwindow]=scores.get_precision()\n",
    "        myRE2[subid,nwindow]=scores.get_recall()\n",
    "            \n",
    "            \n",
    "\n",
    "print(\"--- %s seconds --------\" % (time.time() - start_time))\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "meanFM0=myFM0.mean(axis=0)\n",
    "meanPR0=myPR0.mean(axis=0)\n",
    "meanRE0=myRE0.mean(axis=0)\n",
    "meanFM1=myFM1.mean(axis=0)\n",
    "meanPR1=myPR1.mean(axis=0)\n",
    "meanRE1=myRE1.mean(axis=0)\n",
    "meanFM2=myFM2.mean(axis=0)\n",
    "meanPR2=myPR2.mean(axis=0)\n",
    "meanRE2=myRE2.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIN=10\n",
      "RAW  0.09457152725419418 0.16250524462950044 0.139946534740702\n",
      "NLSS 0.07874094812657187 0.15203729358599155 0.1374194058821018\n",
      "EMBD 0.0796785615876045 0.12095165034759334 0.12618683544110407\n",
      "WIN=8\n",
      "RAW  0.09120202738456545 0.16791995567356452 0.135067592627256\n",
      "NLSS 0.06643914458696144 0.14461087352269153 0.13125616865679143\n",
      "EMBD 0.07396582648964332 0.1261665727003643 0.11958366185245421\n"
     ]
    }
   ],
   "source": [
    "stdFM0=myFM0.std(axis=0)\n",
    "stdPR0=myPR0.std(axis=0)\n",
    "stdRE0=myRE0.std(axis=0)\n",
    "stdFM1=myFM1.std(axis=0)\n",
    "stdPR1=myPR1.std(axis=0)\n",
    "stdRE1=myRE1.std(axis=0)\n",
    "stdFM2=myFM2.std(axis=0)\n",
    "stdPR2=myPR2.std(axis=0)\n",
    "stdRE2=myRE2.std(axis=0)\n",
    "\n",
    "print('WIN='+str(MAXW))\n",
    "print('RAW  ' + str(stdFM0[MAXW-1]) + ' ' + str(stdPR0[MAXW-1]) + ' ' + str(stdRE0[MAXW-1]))\n",
    "print('NLSS ' + str(stdFM1[MAXW-1]) + ' ' + str(stdPR1[MAXW-1]) + ' ' + str(stdRE1[MAXW-1]))\n",
    "print('EMBD ' + str(stdFM2[MAXW-1]) + ' ' + str(stdPR2[MAXW-1]) + ' ' + str(stdRE2[MAXW-1]))\n",
    "\n",
    "print('WIN='+str(bestWin))\n",
    "print('RAW  ' + str(stdFM0[bestWin-1]) + ' ' + str(stdPR0[bestWin-1]) + ' ' + str(stdRE0[bestWin-1]))\n",
    "print('NLSS ' + str(stdFM1[bestWin-1]) + ' ' + str(stdPR1[bestWin-1]) + ' ' + str(stdRE1[bestWin-1]))\n",
    "print('EMBD ' + str(stdFM2[bestWin-1]) + ' ' + str(stdPR2[bestWin-1]) + ' ' + str(stdRE2[bestWin-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 32.47061204910278 seconds --------\n",
      " \n",
      "269.0\n",
      "935.95\n",
      "1752.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE PERFORMANCE OF RAW FEATURES\n",
    "\n",
    "TOL = 5\n",
    "\n",
    "bestIt=1; bestWin=8; MAXW=10;\n",
    "\n",
    "import itertools\n",
    "start_time = time.time()\n",
    "\n",
    "all_scores = {}\n",
    "scores = evaluation_measures(win=5)\n",
    "\n",
    "MAXW=10\n",
    "\n",
    "myFM3=np.zeros( (N_SEQ,MAXW) )\n",
    "myPR3=np.zeros( (N_SEQ,MAXW) )\n",
    "myRE3=np.zeros( (N_SEQ,MAXW) )\n",
    "\n",
    "NN=np.zeros(N_SEQ)\n",
    "\n",
    "for subid in range(N_SEQ): \n",
    "    GT = np.load(path_dir + \"GT/seq_\" + str(subid+1)+\".npy\")\n",
    "    start_time1 = time.time()\n",
    "    \n",
    "    #--- LOAD EDUB features\n",
    "    FEATDIR = ''\n",
    "    FILENAME= FPTH + SNME\n",
    "    FN= SNME + '_seq' + str(subid+1) \n",
    "    dataiter = np.load(FEATDIR + FN + '.npy')\n",
    "    \n",
    "    #print(np.min(dataiter),np.max(dataiter))\n",
    "    \n",
    "    tmp=dataiter.shape\n",
    "    #print(tmp[0])\n",
    "    #dataiter=2*(dataiter-1/2)\n",
    "    NN[subid]=tmp[0]\n",
    "    \n",
    "    for nwindow in range(2,MAXW):\n",
    "        P, F = mean_aggregation(dataiter, N=nwindow)\n",
    "        prediction =  boundary_prediction(P,F)\n",
    "        scores.reset(y_pred=prediction, y_gt=GT, win = TOL)            \n",
    "        myFM3[subid,nwindow]=scores.f_measure()\n",
    "        myPR3[subid,nwindow]=scores.get_precision()\n",
    "        myRE3[subid,nwindow]=scores.get_recall()\n",
    "                \n",
    "            \n",
    "\n",
    "print(\"--- %s seconds --------\" % (time.time() - start_time))\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "print(' ')\n",
    "print(NN.min())\n",
    "print(NN.mean())\n",
    "print(NN.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
