{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as FTR\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "#from dge_fun import *\n",
    "#from kmeans import lloyd\n",
    "\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# FORCE CPU for timing purpose\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "#load cleaned data\n",
    "path_dir = \"\"\n",
    "save_path = \"\"\n",
    "lossdata_path = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- FUNCTION DEFINITONS --> PUT INTO MODULE\n",
    "#--- ? HOW TO PASS ON \"device\" TO MODULE ?\n",
    "\n",
    "def class_sim_torch(X):\n",
    "    Tpxy=torch.sum(torch.abs(X - X[:, None]),2)\n",
    "    return Tpxy\n",
    "\n",
    "def PDIST(X):\n",
    "    #print(X.type())\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    numel=N*N*dim\n",
    "    numel_max = 400000000\n",
    "    #numel_max = 100000000\n",
    "    nl=np.ceil(numel/numel_max).astype(int) # divide dim by this\n",
    "    if nl ==1:\n",
    "        Tpxy=torch.sum(torch.abs(X - X[:, None]),2)\n",
    "    else:\n",
    "        Tpxy=torch.zeros(N,N).to(device)\n",
    "        Tpxy=Tpxy.type(X.type())\n",
    "        mdim=np.floor(dim/nl).astype(int) # dim slices\n",
    "        for i in range(nl+1):\n",
    "            i1=i*mdim\n",
    "            i2=np.minimum((i+1)*mdim,dim)\n",
    "            if i1<i2:\n",
    "                Xt=X[:,i1:i2]\n",
    "                Tpxy.add_(torch.sum(torch.abs(Xt - Xt[:, None]),2))    \n",
    "    return Tpxy\n",
    "\n",
    "def NLSS( X, param,r_loc):\n",
    "#    XSS_ij = exp( -1/param * sum_k |X(i,k) - X(j,k)| / N_features )\n",
    "# size(X)= N_frames x N_features \n",
    "# size(XSS)= N_frames x N_frames \n",
    "#\n",
    "# if r_loc is given, it specifies the \"search radius\" for NLmeans\n",
    "# in this case:\n",
    "#    XSS_ij is exponentially tapered\n",
    "#    XSS_ij = 0 for |i-j|>r_loc\n",
    "#    XSS is periodically extended to size N_frames x N_frames + 2*r_loc\n",
    "    if len(sys.argv)<3:\n",
    "        r_loc=[]\n",
    "        \n",
    "    N = len(X)\n",
    "    dim = X.shape[1]\n",
    "    #--- compute distance & similarity\n",
    "    XM2 = sp.spatial.distance.squareform(sp.spatial.distance.pdist(X,metric = 'cityblock'))/dim\n",
    "    XSS0 = np.exp(-1/param*XM2)\n",
    "\n",
    "    # if not NLM over whole sequence, set similarity beyond search radius to\n",
    "    # zero and extend boundaries\n",
    "    if r_loc:   \n",
    "        t=np.arange(1,N)\n",
    "        for i in range(1,N):\n",
    "            t = np.concatenate((t, np.arange(1,N-i)), axis=None)\n",
    "        t = np.transpose(t)\n",
    "        T=spatial.distance.squareform(t) #%T=T(:,1:min(N1,N2));\n",
    "        MASK=sp.exp(math.log(0.25)*T/r_loc)\n",
    "        XSS=np.multiply(XSS0,MASK)\n",
    "        XSS = np.transpose(XSS)\n",
    "        XSS= np.concatenate((np.flipud(XSS[0:r_loc,:]),XSS,np.flipud(XSS[XSS.shape[0]-r_loc:XSS.shape[0],:])), axis=0)\n",
    "        XSS = np.transpose(XSS)\n",
    "    return XSS\n",
    "\n",
    "def construct_SS_matrix(X,nnh):\n",
    "# function [XX] = construct_SS_matrix(X,nnh)\n",
    "# \n",
    "# stack nnh left-and right neighbor feature vectors of X using periodic\n",
    "# boundary conditions.  \n",
    "# size(X)  = N_frames x N_features \n",
    "# size(XX) = N_frames x N_features* 2*nnh \n",
    "\n",
    "    # periodic border effects\n",
    "    XE=np.concatenate((  np.flipud(X[1:nnh+1,:]),X,(np.flipud(X[X.shape[0]-nnh-2:X.shape[0]-1,:]))), axis = 0)\n",
    "    \n",
    "    #--- construct neighborhood \n",
    "    a= np.arange(-nnh,0, dtype = int)\n",
    "    b = np.arange(1,nnh+1, dtype = int)\n",
    "\n",
    "    NH_ID = np.concatenate((a,b))# do not include self\n",
    "    #NH_ID = np.concatenate((a,0, b))% include self (i.e., \"center pixel\")\n",
    "\n",
    "    if not NH_ID.any():\n",
    "        # nothing to do: working with pixel only, without neighbors\n",
    "        XX=X\n",
    "    else:\n",
    "        # stack neighbors\n",
    "        XX=np.array([]) \n",
    "    for inh in NH_ID:\n",
    "        XTMP= np.roll(XE,inh,axis=0) \n",
    "        if not XX.size:  \n",
    "            XX = XTMP[nnh:XTMP.shape[0]-nnh-1,:]\n",
    "        else:\n",
    "            XX=np.hstack((XX,XTMP[nnh:XTMP.shape[0]-nnh-1,:]))\n",
    "    return XX\n",
    "\n",
    "def NLSS_torch( X, param,r_loc):\n",
    "#    XSS_ij = exp( -1/param * sum_k |X(i,k) - X(j,k)| / N_features )\n",
    "# size(X)= N_frames x N_features \n",
    "# size(XSS)= N_frames x N_frames \n",
    "#\n",
    "# if r_loc is given, it specifies the \"search radius\" for NLmeans\n",
    "# in this case:\n",
    "#    XSS_ij is exponentially tapered\n",
    "#    XSS_ij = 0 for |i-j|>r_loc\n",
    "#    XSS is periodically extended to size N_frames x N_frames + 2*r_loc\n",
    "    if len(sys.argv)<3:\n",
    "        r_loc=[]\n",
    "        \n",
    "    N = X.size()[0]\n",
    "    dim = X.size()[1]\n",
    "    #--- compute distance & similarity\n",
    "    XM2 = PDIST(X)/dim\n",
    "    XSS0 = torch.exp(-1/param*XM2).double()\n",
    "    # if not NLM over whole sequence, set similarity beyond search radius to\n",
    "    # zero and extend boundaries\n",
    "    if r_loc:   \n",
    "        t1=torch.zeros(N,1).to(device)\n",
    "        t1[:,0]=torch.arange(0,N)\n",
    "        T=PDIST(t1)\n",
    "        mparam=-1.3862943611198906/r_loc\n",
    "        MASK=torch.exp(mparam*T).double()\n",
    "        XSS=torch.mul(XSS0,MASK)\n",
    "        XSS = XSS.transpose(1,0)\n",
    "        \n",
    "        np1=torch.flip(XSS[0:r_loc,:],(0,))        \n",
    "        np2=torch.flip(XSS[XSS.size()[0]-r_loc:XSS.size()[0],:],(0,))\n",
    "        XSS = torch.cat((np1, XSS, np2))\n",
    "        XSS = XSS.transpose(1,0)\n",
    "    return XSS\n",
    "\n",
    "def PCA(X, k=2):\n",
    "    # preprocess the data\n",
    "    X_mean = torch.mean(X,0)\n",
    "    X = X - X_mean.expand_as(X)\n",
    "    # svd\n",
    "    U,S,V = torch.svd(torch.t(X))\n",
    "    return U[:,:k], torch.mm(X,U[:,:k])\n",
    "\n",
    "def distx_fun(X):\n",
    "    N = X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "\n",
    "    Z = torch.mm(X,X.t())#.to(device)\n",
    "    djj = torch.sqrt(torch.diag(Z))*torch.ones(1,N).double().t().to(device)\n",
    "    Z = torch.div(1 - torch.div(torch.div(Z,djj.t()), djj) , dim)\n",
    "    return Z\n",
    "    \n",
    "def simz_fun(Z,param):\n",
    "    G = torch.exp(torch.mul(-1/param,Z)) \n",
    "    N=G.size()[1]\n",
    "    z = torch.sum(G,0)\n",
    "    G=torch.div(G,z.repeat(N,1));\n",
    "    return G\n",
    "\n",
    "def norm11(X):\n",
    "    for i in range(0,X.size()[1]):\n",
    "        X[:,i]=X[:,i]-X[:,i].min()\n",
    "        mm=X[:,i].max()\n",
    "        if mm is not 0:\n",
    "            X[:,i]=X[:,i]/mm*2-1\n",
    "    return X\n",
    "\n",
    "def loss_fun(G,W):\n",
    "    N=G.size()[0]\n",
    "    # cross-entropy loss \n",
    "    w=W.contiguous().view(N*N,-1)\n",
    "    g=G.contiguous().view(N*N,-1)\n",
    "    L=-torch.sum(torch.mul(w,torch.log(g)))\n",
    "    L=L+torch.sum(torch.mul(w,torch.log(w)))\n",
    "    return L\n",
    "\n",
    "def d_distx_fun(X):\n",
    "    tt=X.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[1]\n",
    "\n",
    "    Z = torch.mm(X,X.t()) + 1e-16\n",
    "    N = Z.size()[1];\n",
    "    djj = torch.sqrt(torch.diag(Z))*torch.ones(1,N).double().t().to(device)\n",
    "    dii= djj.t()\n",
    "    t1 = torch.div(torch.div(1,dii), djj)\n",
    "    t1 = t1 + torch.diag(torch.diag(t1)) # OK\n",
    "    t2 = t1*torch.div(torch.div(Z,djj), djj) # OK\n",
    "\n",
    "    X1=X\n",
    "    X1.detach().clone()\n",
    "    X1=X1.repeat((N,1,1))\n",
    "    X2=X1\n",
    "    X2.detach().clone()\n",
    "    X2=X2.transpose(0,1)\n",
    "\n",
    "    T1=t1.repeat((dim,1,1))\n",
    "    T1=T1.transpose(0,1).transpose(1,2)\n",
    "    T2=t2.repeat((dim,1,1))\n",
    "    T2=T2.transpose(0,1).transpose(1,2)\n",
    "\n",
    "    #dZ=X2*T1 - X1*T2\n",
    "    dZ=torch.mul(X2,T1) - torch.mul(X1,T2)\n",
    "    dZ=-dZ\n",
    "    return dZ \n",
    "    \n",
    "def d_simz_fun(Z,dZ,param):\n",
    "    tt=dZ.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[2]\n",
    "    G = torch.exp(torch.mul(-1/param,Z))\n",
    "    dG0=-1/param*G\n",
    "    dG0=dG0.repeat((dim,1,1))\n",
    "    dG0=dG0.transpose(0,1).transpose(1,2)\n",
    "    #dG=dZ*dG0\n",
    "    dG=torch.mul(dZ,dG0)\n",
    "\n",
    "    sG=torch.sum(G,0).repeat((N,1)).repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    sdG=torch.sum(dG,0).repeat((N,1,1))\n",
    "\n",
    "    repG=G.repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    dG=torch.div((dG*sG - repG*sdG) , (sG*sG))\n",
    "    return dG\n",
    "\n",
    "def d_loss_fun(G,dG,W):\n",
    "    tt=dG.size()\n",
    "    N=tt[0]\n",
    "    dim=tt[2]\n",
    "    WG=torch.div(W,G).repeat((dim,1,1)).transpose(0,1).transpose(1,2)\n",
    "    dL=-torch.sum(dG*WG,0)\n",
    "    return dL\n",
    "\n",
    "def full_grad_fun(X,W,param):\n",
    "    Z = distx_fun(X)\n",
    "    dZ = d_distx_fun(X)\n",
    "    G = simz_fun(Z,param)\n",
    "    dG = d_simz_fun(Z,dZ,param)\n",
    "    dL = d_loss_fun(G,dG,W)\n",
    "    return dL, G\n",
    "\n",
    "def sim_grad_descent(X,W,param,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    dL,G=full_grad_fun(X,W,param) # compute gradient\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    dataiter=X-eta*dL;\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        dL,G=full_grad_fun(dataiter,W,param) # compute gradient\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=dataiter.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2;\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        dataiter=dataiter - eta*dL\n",
    "    \n",
    "    return dataiter\n",
    "\n",
    "def sim_grad_descent_autograd(X,W,param,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    \n",
    "    loss=loss_fun(simz_fun(distx_fun(X),param),W)\n",
    "    loss.backward()\n",
    "    dL=X.grad\n",
    "\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    X=X-eta*dL\n",
    "    X=X.detach().clone()\n",
    "    X.requires_grad=True\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        loss=loss_fun(simz_fun(distx_fun(X),param),W)\n",
    "        loss.backward()\n",
    "#        dL=X.grad.detach().clone()\n",
    "        dL=X.grad\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=X.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        X=X - eta*dL\n",
    "        X=X.detach().clone()\n",
    "        X.requires_grad=True\n",
    "    \n",
    "    return X\n",
    "\n",
    "def sim_grad_descent_biW(X,W1,W2,param,aa,niter):\n",
    "    eta0=1e-6\n",
    "    aa=min(max(aa,0),1)\n",
    "    a1,a2=aa,1-aa\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    dL1,G1=full_grad_fun(X,W1,param) # compute gradient\n",
    "    dL2,G2=full_grad_fun(X,W2,param) # compute gradient\n",
    "    dL=a1*dL1+a2*dL2\n",
    "    G=a1*G1+a2*G2\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    dataiter=X-eta*dL;\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        dL1,G1=full_grad_fun(dataiter,W1,param) # compute gradient\n",
    "        dL2,G2=full_grad_fun(dataiter,W2,param) # compute gradient\n",
    "        dL=a1*dL1+a2*dL2\n",
    "        G=a1*G1+a2*G2\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=dataiter.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2;\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        dataiter=dataiter - eta*dL\n",
    "    \n",
    "    return dataiter\n",
    "\n",
    "def loss_fun_biW(G,W1,W2,aa):\n",
    "    N=G.size()[0]\n",
    "    aa=min(max(aa,0),1)\n",
    "    a1,a2=aa,1-aa\n",
    "    # cross-entropy loss \n",
    "    w1=W1.contiguous().view(N*N,-1)\n",
    "    w2=W2.contiguous().view(N*N,-1)\n",
    "    g=G.contiguous().view(N*N,-1)\n",
    "    L=-a1*torch.sum(torch.mul(w1,torch.log(g)))-a2*torch.sum(torch.mul(w2,torch.log(g)))\n",
    "    L=L+a1*torch.sum(torch.mul(w1,torch.log(w1)))+a2*torch.sum(torch.mul(w2,torch.log(w2)))\n",
    "    return L\n",
    "\n",
    "def sim_grad_descent_biW_autograd(X,W1,W2,param,aa,niter):\n",
    "    eta0=1e-6\n",
    "    N=X.size()[0]\n",
    "    dim=X.size()[1]\n",
    "    # step 1\n",
    "    \n",
    "    loss=loss_fun_biW(simz_fun(distx_fun(X),param),W1,W2,aa)\n",
    "    loss.backward()\n",
    "    dL=X.grad\n",
    "\n",
    "    x1=X.permute(1,0)\n",
    "    g1=dL.permute(1,0)\n",
    "    x1=x1.contiguous().view(-1,N*dim)\n",
    "    g1=g1.contiguous().view(-1,N*dim)\n",
    "    eta=eta0\n",
    "    X=X-eta*dL\n",
    "    X=X.detach().clone()\n",
    "    X.requires_grad=True\n",
    "\n",
    "    # step 2-niter\n",
    "    for it in range(niter-1):\n",
    "        loss=loss_fun_biW(simz_fun(distx_fun(X),param),W1,W2,aa)\n",
    "        loss.backward()\n",
    "        dL=X.grad\n",
    "        x2=x1\n",
    "        g2=g1\n",
    "        x2.detach().clone()\n",
    "        g2.detach().clone()\n",
    "        x1=X.permute(1,0)\n",
    "        g1=dL.permute(1,0)\n",
    "        x1=x1.contiguous().view(-1,N*dim)\n",
    "        g1=g1.contiguous().view(-1,N*dim)\n",
    "        dx=x1-x2\n",
    "        dg=g1-g2\n",
    "        eta=torch.mm(dg,dx.transpose(1,0)) / torch.mm(dg,dg.transpose(1,0))\n",
    "\n",
    "        if eta != eta: # eta is nan. this actually means that the gradient is zero: STOP\n",
    "            #print(g1)\n",
    "            eta=1e-16\n",
    "        \n",
    "        X=X - eta*dL\n",
    "        X=X.detach().clone()\n",
    "        X.requires_grad=True\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "FPTH = \"/media/DATA/DATASET/EDUB-Seg/Feat_2048/\"\n",
    "SPTH = FPTH + 'NL_SS_FINAL/self_loc_L1_'\n",
    "\n",
    "SAVEDIR = \"\"\n",
    "SAVEDIR = \"\"\n",
    "\n",
    "N_SEQ=20\n",
    "SNME='EDUB-Seg'\n",
    "\n",
    "\n",
    "DO_AUTOGRAD=0 # Use Autograd instead of manual grad\n",
    "DO_PCA=0 # initialize embedding with PCA\n",
    "\n",
    "\n",
    "# master parameter for similarity\n",
    "par00=0.25 \n",
    "\n",
    "# search radius and neighborhood for NLmean\n",
    "r_loc=3\n",
    "nnh=1\n",
    "\n",
    "# parameters for initial embedding\n",
    "param2=par00/0.8\n",
    "param1=par00/50/2\n",
    "d_emb = 15\n",
    "niter_e=150\n",
    "\n",
    "# parameters for dynamic graph embedding\n",
    "N_iter_G=10 # number of DGE iterations\n",
    "niter_g=5  # embedding steps\n",
    "aa=0.3     # proportionality factor for initial matrix W0\n",
    "aa2=1e-3   # strengthen diagonal\n",
    "NCL=9      # number of clusters\n",
    "Cdamp=0.95 # reduction of out-of-cluster similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "aa=0.3; aa2=3e-2; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.975;\n",
    "aa2=3e-1; \n",
    "\n",
    "aa=0.1; aa2=5e-1; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.85;\n",
    "\n",
    "\n",
    "\n",
    "aa=0.4; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=5; Cdamp=0.9; NF=1;\n",
    "#_a0.4_t0.3_NF1_CL10_CLd0.9_ie5\n",
    "\n",
    "aa=0.3; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=3; Cdamp=0.9; NF=2;\n",
    "#_a0.3_t0.3_NF2_CL10_CLd0.9_ie3\n",
    "\n",
    "aa=0.1; aa2=3e-1; N_iter_G=10; NCL=10; niter_g=3; Cdamp=0.9; NF=3;\n",
    "#_a0.1_t0.3_NF3_CL10_CLd0.9_ie3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SAVE_OUT=1\n",
    "\n",
    "#KMEANS_sklearn=0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# FORCE CPU for timing purpose\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "NLSstr= '_NH' + str(nnh) + '_R' + str(r_loc) + '_'+str(par00)\n",
    "embstr = '_emb_dim' + str(d_emb)+'_it' + str(niter_e)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FPTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cdf07e00cb80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#--- LOAD EDUB features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mFILENAME\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFPTH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSNME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mFN\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSNME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_seq'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILENAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_seq'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FPTH' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "\n",
    "\n",
    "\n",
    "for subid in range(20): #range(N_SEQ): \n",
    "    \n",
    "\n",
    "    \n",
    "    #--- LOAD EDUB features\n",
    "    FILENAME= FPTH + SNME\n",
    "    FN= SNME + '_seq' + str(subid+1) \n",
    "    X1 = np.load(FILENAME + '_seq' + str(subid+1) + '.npy')\n",
    "\n",
    "    N = X1.shape[0]; print(FN, ' N =', N)\n",
    "    \n",
    "    #######################################################\n",
    "    #--- PART 1: NLSS : compute similarity and do nonlocal mean\n",
    "    \n",
    "    if DO_PCA==1:\n",
    "        PCAstr = '_PCA'\n",
    "    else:\n",
    "        PCAstr = ''\n",
    "        \n",
    "    if DO_AUTOGRAD==1:\n",
    "        GST='_auto'\n",
    "    else:\n",
    "        GST='_man'\n",
    "\n",
    "    try:\n",
    "        FN1 = SAVEDIR + FN + NLSstr + '.npy'; \n",
    "        dataiter = np.load(FN1)\n",
    "    except:\n",
    "        # - compute (self)-similarity \n",
    "        X1=X1-X1.min(); X1=X1/X1.max()*2-1 #normalized to [-1,1]\n",
    "        XXT=torch.tensor(construct_SS_matrix(X1,nnh)).to(device) # stack neighborhood\n",
    "        XSS = NLSS_torch(XXT,par00,r_loc).cpu().detach().numpy() # self-similarity\n",
    "        X1r=np.concatenate((np.flipud(X1[0:r_loc,:]),X1, np.flipud(X1[X1.shape[0]-r_loc:X1.shape[0],:]))) # pad with search radius (periodic boundary conditions)\n",
    "\n",
    "        # - compute non-local mean\n",
    "        a = np.matmul(np.asarray(XSS),np.asarray(X1r))\n",
    "        xss = np.transpose(XSS)\n",
    "        b = np.diag(1./xss.sum(axis=0))\n",
    "        dataiter=np.matmul(b,a) # non-local mean\n",
    "        dataiter=dataiter-dataiter.min(); dataiter=dataiter/dataiter.max()*2-1 #----- normalize to [-1,1];\n",
    "\n",
    "        if SAVE_OUT==1:\n",
    "            FN1 = SAVEDIR + FN + NLSstr + '.npy'; \n",
    "            np.save(FN1,dataiter)\n",
    "        #        disave=norm11(dataiter); np.save(FN1,disave)\n",
    "    \n",
    "    # - initial similarity of cleaned data\n",
    "    dataiter = torch.from_numpy(dataiter).to(device)\n",
    "    dataiter = norm11(dataiter)\n",
    "    W1 = simz_fun(distx_fun(dataiter),param1)\n",
    "        \n",
    "    #######################################################\n",
    "    #--- PART 2: EMBEDDING\n",
    "    try:\n",
    "#        # PROBLEM WITH PCA\n",
    "#        aaa\n",
    "        FN2 = SAVEDIR + FN + NLSstr + embstr + GST + PCAstr + '.npy'; \n",
    "        dataiter = np.load(FN2)\n",
    "        dataiter = torch.from_numpy(dataiter).to(device)\n",
    "    except:\n",
    "        # - send cleaned data to device and normalize each feature\n",
    "        #dataiter = torch.from_numpy(dataiter).to(device)\n",
    "        dataiter = norm11(dataiter)\n",
    "\n",
    "        # - initial similarity of cleaned data\n",
    "        W1 = simz_fun(distx_fun(dataiter),param1) # OK\n",
    "\n",
    "        # - initialize\n",
    "        if DO_PCA==1:\n",
    "            LF, dataiter = PCA(dataiter,d_emb)\n",
    "        dataiter = norm11(dataiter[:,:d_emb])\n",
    "\n",
    "        # - 1st order LINE\n",
    "        #dataiter.cpu().detach().numpy()\n",
    "        if DO_AUTOGRAD==1:\n",
    "            dataiter.requires_grad=True # required for AUTOGRAD\n",
    "            dataiter = sim_grad_descent_autograd(dataiter,W1,param2,niter_e)\n",
    "            dataiter=dataiter.detach()\n",
    "            GST='_auto'\n",
    "        else:\n",
    "            dataiter = sim_grad_descent(dataiter,W1,param2,niter_e)\n",
    "            GST='_man'\n",
    "\n",
    "        if SAVE_OUT==1:\n",
    "            FN2 = SAVEDIR + FN + NLSstr + embstr + GST + PCAstr + '.npy'; np.save(FN2,(dataiter).cpu().detach().numpy())\n",
    "        \n",
    "    FN1 = SAVEDIR + FN + NLSstr + '.npy'\n",
    "    FN2 = SAVEDIR + FN + NLSstr + embstr + GST + PCAstr + '.npy'\n",
    "    print(FN1)\n",
    "    print(FN2)\n",
    "        \n",
    "    \n",
    "    #######################################################\n",
    "    #--- PART 3: DGE LOOP\n",
    "    \n",
    "    # - normalize and initialize\n",
    "    dataiter = norm11(dataiter).double()\n",
    "    #--- initial data similarity\n",
    "    W2 = simz_fun(distx_fun(dataiter),param2)\n",
    "    \n",
    "    #######################################################\n",
    "    y = np.linspace(1, N, N)\n",
    "    xv, yv = np.meshgrid(y, y)\n",
    "    id1=np.where(abs(xv-yv)>1)\n",
    "    id2=np.where(abs(xv-yv)<2)\n",
    "    #######################################################\n",
    "    \n",
    "    #######################################################\n",
    "    N=dataiter.shape[0]\n",
    "    y = np.linspace(1, N, N)\n",
    "    xv, yv = np.meshgrid(y, y)\n",
    "    id1=np.where(abs(xv-yv)>1)\n",
    "    id2=np.where(abs(xv-yv)<2)\n",
    "    iNF=int(NF)\n",
    "    hNF=int(NF/2)\n",
    "    FF= Variable(torch.ones(1, 1, iNF, iNF)/(NF*NF)).double().to(device)\n",
    "#                F=np.ones((int(NF),int(NF)))/(NF*NF)\n",
    "    #######################################################\n",
    "    \n",
    "    for nDGE in range(N_iter_G):\n",
    "        \n",
    "        #########################################################################\n",
    "        ####### !!!!! CHANGED TEMPORAL REGULARIZATION !!!!! HW 22/07/2019 #######\n",
    "        #########################################################################\n",
    "\n",
    "        #--- 0 modify W: temporal smoothing\n",
    "        if int(NF)>1:\n",
    "            W = torch.zeros(1, 1, N+iNF, N+iNF).double().to(device)\n",
    "            W[0,0,hNF:N+hNF,hNF:N+hNF]=W2\n",
    "            W2 = FTR.conv2d(W, FF)[0,0,:N,:N].double()\n",
    "#            F=np.ones((NF,NF))/(NF*NF)\n",
    "#            W2= torch.from_numpy(signal.convolve2d(W2.cpu().detach().numpy(), F, boundary='symm', mode='same')).to(device)\n",
    "        \n",
    "        #--- 0 modify W: strengthen diagonal\n",
    "        #W2=W2*(torch.eye(W2.size()[0]).double().to(device)*aa2 + torch.ones(W2.size()[0]).double().to(device)*(1-aa2))\n",
    "        W2[id1[0],id1[1]]=(1-aa2)*W2[id1[0],id1[1]]\n",
    "        \n",
    "\n",
    "        #%W=diag(diag(W))+(1-aa2)*(W-diag(diag(W)));\n",
    "        #[i1,i2]=meshgrid(1:N); W(abs(i1-i2)>1)= (1-aa2)*W(abs(i1-i2)>1);\n",
    "        \n",
    "        #--- 1 update embedding + sim matrix\n",
    "        if DO_AUTOGRAD==1:\n",
    "            dataiter.requires_grad=True\n",
    "            dataiter = sim_grad_descent_biW_autograd(dataiter,W1,W2,param2,aa,niter_g).detach().clone()\n",
    "        else:\n",
    "            dataiter = sim_grad_descent_biW(dataiter,W1,W2,param2,aa,niter_g)\n",
    "        dataiter = norm11(dataiter)\n",
    "        \n",
    "        #--- 2 compute new W\n",
    "        Z = distx_fun(dataiter) \n",
    "        W2 = simz_fun(Z,param2)\n",
    "        \n",
    "        #--- 3 modify W from clustering\n",
    "        #kmeans = KMeans(n_clusters=NCL, random_state=0).fit(dataiter.cpu().numpy())\n",
    "        kmeans = KMeans(n_clusters=NCL, random_state=0, init='k-means++').fit(dataiter.cpu().numpy())\n",
    "        #kmeans = KMeans(n_clusters=NCL).fit(dataiter.cpu().numpy()) ## RANDOM ##\n",
    "        CC0=torch.tensor([kmeans.labels_]).transpose(1,0)\n",
    "\n",
    "        CIM = class_sim_torch(CC0).to(device)\n",
    "        CIM = (CIM==0).double() + Cdamp*(CIM!=0).double()\n",
    "        W2=W2*CIM\n",
    "        \n",
    "        if SAVE_OUT==1:\n",
    "            FN3 = SAVEDIR + FN + NLSstr + embstr + GST + PCAstr + '_it' + str(nDGE+1) + '_DGE.npy'; \n",
    "            if iNF>1:\n",
    "                FN3 = SAVEDIR + FN + NLSstr + embstr + GST +'_NF' + str(iNF) + PCAstr + '_it' + str(nDGE+1) + '_DGE.npy'; \n",
    "            np.save(FN3,dataiter.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    \n",
    "    #######################################################\n",
    "\n",
    "    # --- save\n",
    "    \n",
    "#    np.save(SPTH+SaveStr,dataiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NF=3\n",
    "F=np.ones((NF,NF))/(NF*NF)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "N = 5\n",
    "NF=3.0\n",
    "\n",
    "iNF=int(NF)\n",
    "hNF=int(NF/2)\n",
    "W0 = torch.ones( N, N)\n",
    "W = torch.zeros(1, 1, N+iNF, N+iNF)\n",
    "W[0,0,hNF:N+hNF,hNF:N+hNF]=W2\n",
    "\n",
    "FF= Variable(torch.ones(1, 1, iNF, iNF) )#/(NF*NF))\n",
    "W2 = F.conv2d(W, FF)[0,0,:N,:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
